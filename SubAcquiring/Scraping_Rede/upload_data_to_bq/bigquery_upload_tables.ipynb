{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd636f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import unicodedata\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "RAW_FILES_DIR = './raw_files'\n",
    "EXTRACTED_DIR = './extracted_files'\n",
    "CSV_OUTPUT_DIR = './csv_output'\n",
    "\n",
    "os.makedirs(EXTRACTED_DIR, exist_ok=True)\n",
    "os.makedirs(CSV_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# List all ZIP files in the raw_files directory\n",
    "zip_files = glob.glob(os.path.join(RAW_FILES_DIR, '*.zip'))\n",
    "print(f\"Found {len(zip_files)} ZIP files.\")\n",
    "\n",
    "# Today\n",
    "today = datetime.datetime.today()\n",
    "# today = today + datetime.timedelta(days=-1)\n",
    "today = today.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13321886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompress all ZIP files\n",
    "for zip_path in zip_files:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACTED_DIR)\n",
    "print(f\"All files extracted to {EXTRACTED_DIR}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate files by prefix and save to unique CSVs in ./csv_output (full join on columns)\n",
    "\n",
    "# Find all files in the extracted_files directory\n",
    "extracted_files = glob.glob(os.path.join(EXTRACTED_DIR, '*'))\n",
    "prefix_groups = defaultdict(list)\n",
    "\n",
    "# Group files by prefix (before '-')\n",
    "for file_path in extracted_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    match = re.match(r'([^\\-]+)-', filename)\n",
    "    if match:\n",
    "        prefix = match.group(1)\n",
    "        prefix_groups[prefix].append(file_path)\n",
    "\n",
    "# For each prefix, concatenate all files and save as prefix.csv (full join on columns)\n",
    "for prefix, files in prefix_groups.items():\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            if f.endswith('.csv'):\n",
    "                df = pd.read_csv(f, dtype=str, encoding='utf-8', sep=';')\n",
    "            elif f.endswith('.xlsx'):\n",
    "                df = pd.read_excel(f, dtype=str)\n",
    "            else:\n",
    "                continue\n",
    "            # Normalize column names: remove accents and replace special characters\n",
    "            table_names_path = os.path.join(os.path.dirname(__file__), 'table_names.json') if '__file__' in globals() else './table_names.json'\n",
    "            with open(table_names_path, 'r') as f_table_names:\n",
    "                table_names = json.load(f_table_names)\n",
    "            def normalize_col(col):\n",
    "                col = unicodedata.normalize('NFKD', col).encode('ASCII', 'ignore').decode('ASCII')\n",
    "                col = col.replace('%', 'percent')\n",
    "                col = col.replace(' ', '_')\n",
    "                col = col.replace('/', '_')\n",
    "                col = col.replace('-', '_')\n",
    "                col = col.replace('.', '_')\n",
    "                return col\n",
    "            df.columns = [normalize_col(c) for c in df.columns]\n",
    "            # Convert decimal separator from ',' to '.' for all string columns\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == object:\n",
    "                    df[col] = df[col].str.replace('.', '', regex=False)\n",
    "                    df[col] = df[col].str.replace(',', '.', regex=False)\n",
    "            # Convert DATE columns to date format if schema is available\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {f}: {e}\")\n",
    "    if dfs:\n",
    "        try:\n",
    "            # Use outer join to ensure all columns are included\n",
    "            combined = pd.concat(dfs, ignore_index=True, sort=True)\n",
    "            if len(combined) == 0:\n",
    "                print(f\"No rows to save for prefix {prefix}, skipping CSV output.\")\n",
    "                continue\n",
    "            if prefix == 'capa':\n",
    "                print(f\"Skipping CSV output for prefix 'capa'.\")\n",
    "                continue\n",
    "\n",
    "            prefix_lc = prefix.lower()\n",
    "            if prefix_lc in table_names:\n",
    "                schema_file = os.path.join(os.path.dirname(table_names_path), 'table_schemas', f\"{table_names[prefix_lc]}.json\")\n",
    "                try:\n",
    "                    with open(schema_file, 'r') as f_schema:\n",
    "                        schema = json.load(f_schema)\n",
    "                    date_cols = [col['name'] for col in schema if col['type'] == 'DATE']\n",
    "                    for date_col in date_cols:\n",
    "                        if date_col in combined.columns:\n",
    "                            # print(pd.to_datetime(df[date_col], dayfirst = True, errors='coerce').dt.strftime('%Y-%m-%d'))\n",
    "                            # print(date_col)\n",
    "\n",
    "                            combined[date_col] = pd.to_datetime(combined[date_col], dayfirst = True, errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "                            # print(f\"Converted column {date_col} to date format for prefix {prefix}.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not process schema for {prefix}: {e}\")\n",
    "\n",
    "            combined['updated_on'] = today  # Add updated_on column\n",
    "            # Reorder columns so 'updated_on' is first\n",
    "            cols = ['updated_on'] + [col for col in combined.columns if col != 'updated_on']\n",
    "            combined = combined[cols]\n",
    "            out_path = os.path.join(CSV_OUTPUT_DIR, f'{prefix}.csv')\n",
    "            combined.to_csv(out_path, index=False, encoding='utf-8', sep=';')\n",
    "            print(f\"Saved {out_path} with {len(combined)} rows and {len(list(combined.columns))} columns: {list(combined.columns)}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving {prefix}: {e}\")\n",
    "    else:\n",
    "        print(f\"No data for prefix {prefix}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192ed3f",
   "metadata": {},
   "source": [
    "### Atenção: precisa revisar essa célula e mudar o procedimento dela para pegar os dados das tabelas no BQ, comparar as mudanças, e atualizar os novos. lembrar de particionar as tabelas se elas forem criadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3619dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Essa parte serve para subir as tabelas pela primeira vez\n",
    "## Essa parte ainda não foi implementada e testada\n",
    "\n",
    "# Upload each CSV in csv_output to BigQuery as a table in the Finance dataset\n",
    "\n",
    "# Set your GCP project and dataset\n",
    "GCP_PROJECT = 'api-ms-data'\n",
    "BQ_DATASET = 'Finance'\n",
    "CREDENTIALS_PATH = '../Credentials/your_service_account.json'\n",
    "\n",
    "# Authenticate and create BigQuery client\n",
    "credentials = service_account.Credentials.from_service_account_file(CREDENTIALS_PATH)\n",
    "bq_client = bigquery.Client(credentials=credentials, project=GCP_PROJECT)\n",
    "\n",
    "# List all CSV files in the output directory\n",
    "csv_files = glob.glob(os.path.join(CSV_OUTPUT_DIR, '*.csv'))\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    # Extract table name from file name\n",
    "    base = os.path.basename(csv_path)\n",
    "    name, _ = os.path.splitext(base)\n",
    "    table_id = f\"{GCP_PROJECT}.{BQ_DATASET}.Tbl_SubAdquirencia_Rede_{name}\"\n",
    "    print(f\"Uploading {csv_path} to {table_id} ...\")\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,\n",
    "        autodetect=True,\n",
    "        field_delimiter=';',\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE  # Overwrite table if exists\n",
    "    )\n",
    "    with open(csv_path, \"rb\") as source_file:\n",
    "        load_job = bq_client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "        load_job.result()  # Wait for the job to complete\n",
    "    print(f\"Loaded data to {table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e01d75c",
   "metadata": {},
   "source": [
    "#### Essa parte já está pacificada, não precisa de revisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0295b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all files in raw_files, extracted_files, and csv_output except .gitignore\n",
    "\n",
    "folders = [\"./raw_files\", \"./extracted_files\", \"./csv_output\"]\n",
    "\n",
    "for folder in folders:\n",
    "    files = glob.glob(os.path.join(folder, \"*\"))\n",
    "    for f in files:\n",
    "        if not f.endswith(\".gitignore\"):\n",
    "            try:\n",
    "                os.remove(f)\n",
    "                # print(f\"Deleted: {f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not delete {f}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
